{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5r39wYqtDLJQXeCgsXoJ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkuzdatas/Pr3-CUDA-Colab/blob/main/Lab3-Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6izqQEIZyhFE"
      },
      "source": [
        "This is attempt to run LAB 3 - CNN parameters tunning on GPU with CUDA enabled pytorch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLF2ujRHzWWl"
      },
      "source": [
        "\"\"\"Training utilities.\"\"\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    \"\"\"A custom layer that views an input as 1D.\"\"\"\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "def batchify_data(x_data, y_data, batch_size):\n",
        "    \"\"\"Takes a set of data points and labels and groups them into batches.\"\"\"\n",
        "    # Only take batch_size chunks (i.e. drop the remainder)\n",
        "    N = int(len(x_data) / batch_size) * batch_size\n",
        "    batches = []\n",
        "    for i in range(0, N, batch_size):\n",
        "        batches.append({\n",
        "            'x': torch.tensor(x_data[i:i + batch_size],\n",
        "                              dtype=torch.float32),\n",
        "            'y': torch.tensor([y_data[0][i:i + batch_size],\n",
        "                               y_data[1][i:i + batch_size]],\n",
        "                               dtype=torch.int64)\n",
        "        })\n",
        "    return batches\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, y):\n",
        "    \"\"\"Computes the accuracy of predictions against the gold labels, y.\"\"\"\n",
        "    return np.mean(np.equal(predictions.numpy(), y.numpy()))\n",
        "\n",
        "\n",
        "#def train_model(train_data, dev_data, model, lr=0.01, momentum=0.9, nesterov=False, n_epochs=30):\n",
        "# changed Nestrov to True\n",
        "def train_model(train_data, dev_data, model, lr=0.01, momentum=0.9, nesterov=True, n_epochs=30):\n",
        "    \"\"\"Train a model for N epochs given data and hyper-params.\"\"\"\n",
        "    # We optimize with SGD\n",
        "#    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, nesterov=nesterov)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, nesterov=nesterov)\n",
        "\n",
        "    #Changed to ADAM\n",
        "#    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        print(\"-------------\\nEpoch {}:\\n\".format(epoch))\n",
        "\n",
        "        # Run **training***\n",
        "        loss, acc = run_epoch(train_data, model.train(), optimizer)\n",
        "        print('Train | loss1: {:.6f}  accuracy1: {:.6f} | loss2: {:.6f}  accuracy2: {:.6f}'.format(loss[0], acc[0], loss[1], acc[1]))\n",
        "\n",
        "        # Run **validation**\n",
        "        val_loss, val_acc = run_epoch(dev_data, model.eval(), optimizer)\n",
        "        print('Valid | loss1: {:.6f}  accuracy1: {:.6f} | loss2: {:.6f}  accuracy2: {:.6f}'.format(val_loss[0], val_acc[0], val_loss[1], val_acc[1]))\n",
        "\n",
        "        # Save model\n",
        "        torch.save(model, 'mnist_model_fully_connected.pt')\n",
        "\n",
        "\n",
        "def run_epoch(data, model, optimizer):\n",
        "    \"\"\"Train model for one pass of train data, and return loss, acccuracy\"\"\"\n",
        "    # Gather losses\n",
        "    losses_first_label = []\n",
        "    losses_second_label = []\n",
        "    batch_accuracies_first = []\n",
        "    batch_accuracies_second = []\n",
        "\n",
        "    # If model is in train mode, use optimizer.\n",
        "    is_training = model.training\n",
        "\n",
        "    # Iterate through batches\n",
        "    for batch in tqdm(data):\n",
        "        # Grab x and y\n",
        "        x, y = batch['x'], batch['y']\n",
        "\n",
        "        # Get output predictions for both the upper and lower numbers\n",
        "        out1, out2 = model(x)\n",
        "\n",
        "        # Predict and store accuracy\n",
        "        predictions_first_label = torch.argmax(out1, dim=1)\n",
        "        predictions_second_label = torch.argmax(out2, dim=1)\n",
        "        batch_accuracies_first.append(compute_accuracy(predictions_first_label, y[0]))\n",
        "        batch_accuracies_second.append(compute_accuracy(predictions_second_label, y[1]))\n",
        "\n",
        "        # Compute both losses\n",
        "        loss1 = F.cross_entropy(out1, y[0])\n",
        "        loss2 = F.cross_entropy(out2, y[1])\n",
        "        losses_first_label.append(loss1.data.item())\n",
        "        losses_second_label.append(loss2.data.item())\n",
        "\n",
        "        # If training, do an update.\n",
        "        if is_training:\n",
        "            optimizer.zero_grad()\n",
        "            joint_loss = 0.5 * (loss1 + loss2)\n",
        "            joint_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Calculate epoch level scores\n",
        "    avg_loss = np.mean(losses_first_label), np.mean(losses_second_label)\n",
        "    avg_accuracy = np.mean(batch_accuracies_first), np.mean(batch_accuracies_second)\n",
        "    return avg_loss, avg_accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S54InAPYyu-M",
        "outputId": "40bafc99-5b8a-483c-fd8b-ed117b532716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from train_utils import batchify_data, run_epoch, train_model, Flatten\n",
        "import utils_multiMNIST as U\n",
        "\n",
        "#import os\n",
        "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "path_to_data_dir = '../Datasets/'\n",
        "use_mini_dataset = True\n",
        "\n",
        "batch_size = 64\n",
        "nb_classes = 10\n",
        "nb_epoch = 30\n",
        "num_classes = 10\n",
        "img_rows, img_cols = 42, 28 # input image dimensions\n",
        "\n",
        "\n",
        "import gzip, _pickle, numpy as np\n",
        "num_classes = 10\n",
        "img_rows, img_cols = 42, 28\n",
        "\n",
        "def get_data(path_to_data_dir, use_mini_dataset):\n",
        "\tif use_mini_dataset:\n",
        "\t\texten = '_mini'\n",
        "\telse:\n",
        "\t\texten = ''\n",
        "\tf = gzip.open(path_to_data_dir + 'train_multi_digit' + exten + '.pkl.gz', 'rb')\n",
        "\tX_train = _pickle.load(f, encoding='latin1')\n",
        "\tf.close()\n",
        "\tX_train =  np.reshape(X_train, (len(X_train), 1, img_rows, img_cols))\n",
        "\tf = gzip.open(path_to_data_dir + 'test_multi_digit' + exten +'.pkl.gz', 'rb')\n",
        "\tX_test = _pickle.load(f, encoding='latin1')\n",
        "\tf.close()\n",
        "\tX_test =  np.reshape(X_test, (len(X_test),1, img_rows, img_cols))\n",
        "\tf = gzip.open(path_to_data_dir + 'train_labels' + exten +'.txt.gz', 'rb')\n",
        "\ty_train = np.loadtxt(f)\n",
        "\tf.close()\n",
        "\tf = gzip.open(path_to_data_dir +'test_labels' + exten + '.txt.gz', 'rb')\n",
        "\ty_test = np.loadtxt(f)\n",
        "\tf.close()\n",
        "\treturn X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "\"\"\"Training utilities.\"\"\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    \"\"\"A custom layer that views an input as 1D.\"\"\"\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "def batchify_data(x_data, y_data, batch_size):\n",
        "    \"\"\"Takes a set of data points and labels and groups them into batches.\"\"\"\n",
        "    # Only take batch_size chunks (i.e. drop the remainder)\n",
        "    N = int(len(x_data) / batch_size) * batch_size\n",
        "    batches = []\n",
        "    for i in range(0, N, batch_size):\n",
        "        batches.append({\n",
        "            'x': torch.tensor(x_data[i:i + batch_size],\n",
        "                              dtype=torch.float32),\n",
        "            'y': torch.tensor([y_data[0][i:i + batch_size],\n",
        "                               y_data[1][i:i + batch_size]],\n",
        "                               dtype=torch.int64)\n",
        "        })\n",
        "    return batches\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, y):\n",
        "    \"\"\"Computes the accuracy of predictions against the gold labels, y.\"\"\"\n",
        "    return np.mean(np.equal(predictions.numpy(), y.numpy()))\n",
        "\n",
        "\n",
        "#def train_model(train_data, dev_data, model, lr=0.01, momentum=0.9, nesterov=False, n_epochs=30):\n",
        "# changed Nestrov to True\n",
        "def train_model(train_data, dev_data, model, lr=0.01, momentum=0.9, nesterov=True, n_epochs=30):\n",
        "    \"\"\"Train a model for N epochs given data and hyper-params.\"\"\"\n",
        "    # We optimize with SGD\n",
        "#    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, nesterov=nesterov)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, nesterov=nesterov)\n",
        "\n",
        "    #Changed to ADAM\n",
        "#    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        print(\"-------------\\nEpoch {}:\\n\".format(epoch))\n",
        "\n",
        "        # Run **training***\n",
        "        loss, acc = run_epoch(train_data, model.train(), optimizer)\n",
        "        print('Train | loss1: {:.6f}  accuracy1: {:.6f} | loss2: {:.6f}  accuracy2: {:.6f}'.format(loss[0], acc[0], loss[1], acc[1]))\n",
        "\n",
        "        # Run **validation**\n",
        "        val_loss, val_acc = run_epoch(dev_data, model.eval(), optimizer)\n",
        "        print('Valid | loss1: {:.6f}  accuracy1: {:.6f} | loss2: {:.6f}  accuracy2: {:.6f}'.format(val_loss[0], val_acc[0], val_loss[1], val_acc[1]))\n",
        "\n",
        "        # Save model\n",
        "        torch.save(model, 'mnist_model_fully_connected.pt')\n",
        "\n",
        "\n",
        "def run_epoch(data, model, optimizer):\n",
        "    \"\"\"Train model for one pass of train data, and return loss, acccuracy\"\"\"\n",
        "    # Gather losses\n",
        "    losses_first_label = []\n",
        "    losses_second_label = []\n",
        "    batch_accuracies_first = []\n",
        "    batch_accuracies_second = []\n",
        "\n",
        "    # If model is in train mode, use optimizer.\n",
        "    is_training = model.training\n",
        "\n",
        "    # Iterate through batches\n",
        "    for batch in tqdm(data):\n",
        "        # Grab x and y\n",
        "        x, y = batch['x'], batch['y']\n",
        "\n",
        "        # Get output predictions for both the upper and lower numbers\n",
        "        out1, out2 = model(x)\n",
        "\n",
        "        # Predict and store accuracy\n",
        "        predictions_first_label = torch.argmax(out1, dim=1)\n",
        "        predictions_second_label = torch.argmax(out2, dim=1)\n",
        "        batch_accuracies_first.append(compute_accuracy(predictions_first_label, y[0]))\n",
        "        batch_accuracies_second.append(compute_accuracy(predictions_second_label, y[1]))\n",
        "\n",
        "        # Compute both losses\n",
        "        loss1 = F.cross_entropy(out1, y[0])\n",
        "        loss2 = F.cross_entropy(out2, y[1])\n",
        "        losses_first_label.append(loss1.data.item())\n",
        "        losses_second_label.append(loss2.data.item())\n",
        "\n",
        "        # If training, do an update.\n",
        "        if is_training:\n",
        "            optimizer.zero_grad()\n",
        "            joint_loss = 0.5 * (loss1 + loss2)\n",
        "            joint_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Calculate epoch level scores\n",
        "    avg_loss = np.mean(losses_first_label), np.mean(losses_second_label)\n",
        "    avg_accuracy = np.mean(batch_accuracies_first), np.mean(batch_accuracies_second)\n",
        "    return avg_loss, avg_accuracy\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dimension):\n",
        "        super(CNN, self).__init__()\n",
        "        # TODO initialize model layers here\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1,32,kernel_size=(3,3),padding=1) #,stride=2)\n",
        "        self.conv2 = nn.Conv2d(32,40,kernel_size=(3,3),padding=1)\n",
        "        self.conv3 = nn.Conv2d(40,40,kernel_size=(3,3),padding=1)\n",
        "        self.conv4 = nn.Conv2d(40,32,kernel_size=(3,3),padding=1)\n",
        "        self.dropout = nn.Dropout2d(0.4)\n",
        "        self.fc1 = nn.Linear(64,10)\n",
        "        self.fc2 = nn.Linear(64,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO use model layers to predict the two digits\n",
        "#        x = x.view(-1,1,42,28)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x,(2,2))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x,(2,2))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x,(2,2))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.max_pool2d(x,(2,2))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x1 = self.fc1(x)\n",
        "        x2 = self.fc2(x)\n",
        "        x1 = F.leaky_relu(x1)\n",
        "        x2 = F.leaky_relu(x2)\n",
        "        out_first_digit = x1\n",
        "        out_second_digit = x2\n",
        "\n",
        "        return out_first_digit, out_second_digit\n",
        "\n",
        "def main():\n",
        "    X_train, y_train, X_test, y_test = U.get_data(path_to_data_dir, use_mini_dataset)\n",
        "\n",
        "    # Split into train and dev\n",
        "    dev_split_index = int(9 * len(X_train) / 10)\n",
        "    X_dev = X_train[dev_split_index:]\n",
        "    y_dev = [y_train[0][dev_split_index:], y_train[1][dev_split_index:]]\n",
        "    X_train = X_train[:dev_split_index]\n",
        "    y_train = [y_train[0][:dev_split_index], y_train[1][:dev_split_index]]\n",
        "\n",
        "    permutation = np.array([i for i in range(len(X_train))])\n",
        "    np.random.shuffle(permutation)\n",
        "    X_train = [X_train[i] for i in permutation]\n",
        "    y_train = [[y_train[0][i] for i in permutation], [y_train[1][i] for i in permutation]]\n",
        "\n",
        "    # Split dataset into batches\n",
        "    train_batches = batchify_data(X_train, y_train, batch_size)\n",
        "    dev_batches = batchify_data(X_dev, y_dev, batch_size)\n",
        "    test_batches = batchify_data(X_test, y_test, batch_size)\n",
        "\n",
        "    # Load model\n",
        "    input_dimension = img_rows * img_cols\n",
        "    model = CNN(input_dimension) # TODO add proper layers to CNN class above\n",
        "\n",
        "    # Train\n",
        "    train_model(train_batches, dev_batches, model)\n",
        "\n",
        "    ## Evaluate the model on test data\n",
        "    loss, acc = run_epoch(test_batches, model.eval(), None)\n",
        "    print('Test loss1: {:.6f}  accuracy1: {:.6f}  loss2: {:.6f}   accuracy2: {:.6f}'.format(loss[0], acc[0], loss[1], acc[1]))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Specify seed for deterministic behavior, then shuffle. Do not change seed for official submissions to edx\n",
        "    np.random.seed(12321)  # for reproducibility\n",
        "    torch.manual_seed(12321)  # for reproducibility\n",
        "    main()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-523108d5b56f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from train_utils import batchify_data, run_epoch, train_model, Flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils_multiMNIST\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#import os\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils_multiMNIST'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}